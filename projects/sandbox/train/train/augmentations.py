from typing import Optional

import torch
from torchaudio.transforms import Spectrogram

from ml4gw import gw
from ml4gw.distributions import PowerLaw


class ChannelSwapper(torch.nn.Module):
    """
    Data augmentation module that randomly swaps channels
    of a fraction of batch elements.

    Args:
        frac:
            Fraction of batch that will have channels swapped.
    """

    def __init__(self, frac: float = 0.5):
        super().__init__()
        self.frac = frac

    def forward(self, X):
        num = int(X.shape[0] * self.frac)
        indices = []
        if num > 0:
            num = num if not num % 2 else num - 1
            num = max(2, num)
            channel = torch.randint(X.shape[1], size=(num // 2,)).repeat(2)
            # swap channels from the first num / 2 elements with the
            # second num / 2 elements
            indices = torch.arange(num)
            target_indices = torch.roll(indices, shifts=num // 2, dims=0)
            X[indices, channel] = X[target_indices, channel]
        return X, indices


class ChannelMuter(torch.nn.Module):
    """
    Data augmentation module that randomly mutes 1 channel
    of a fraction of batch elements.

    Args:
        frac:
            Fraction of batch that will have channels muted.
    """

    def __init__(self, frac: float = 0.5):
        super().__init__()
        self.frac = frac

    def forward(self, X):
        num = int(X.shape[0] * self.frac)
        indices = []
        if num > 0:
            channel = torch.randint(X.shape[1], size=(num,))
            indices = torch.randint(X.shape[0], size=(num,))
            X[indices, channel] = torch.zeros(X.shape[-1], device=X.device)

        return X, indices


class SignalInverter(torch.nn.Module):
    """
    Data augmentation that randomly inverts data in a kernel

    Args:
        prob:
            Probability that a kernel is inverted
    """

    def __init__(self, prob: float = 0.5):
        super().__init__()
        self.prob = prob

    def forward(self, X):
        if self.training:
            mask = torch.rand(size=X.shape[:-1]) < self.prob
            X[mask] *= -1
        return X


class SignalReverser(torch.nn.Module):
    """
    Data augmentation that randomly reverses data in a kernel

    Args:
        prob:
            Probability that a kernel is reversed
    """

    def __init__(self, prob: float = 0.5):
        super().__init__()
        self.prob = prob

    def forward(self, X):
        if self.training:
            mask = torch.rand(size=X.shape[:-1]) < self.prob
            X[mask] = X[mask].flip(-1)
        return X


class SnrRescaler(torch.nn.Module):
    """
    Module that calculates SNRs of injections relative
    to a given ASD and performs augmentation of the waveform
    dataset by rescaling injections such that they have SNRs
    given by `target_snrs`. If this argument is `None`, each
    injection is randomly matched with and scaled to the SNR
    of a different injection from the batch.
    """

    def __init__(
        self,
        sample_rate: float,
        waveform_duration: float,
        highpass: Optional[float] = None,
    ):
        super().__init__()
        self.highpass = highpass
        self.sample_rate = sample_rate
        self.df = 1 / waveform_duration
        waveform_size = int(waveform_duration * sample_rate)

        if highpass is not None:
            freqs = torch.fft.rfftfreq(waveform_size, 1 / sample_rate)
            self.register_buffer("mask", freqs >= highpass, persistent=False)
        else:
            self.mask = None

    def forward(
        self,
        responses: gw.WaveformTensor,
        asds: torch.Tensor,
        target_snrs: Optional[gw.ScalarTensor] = None,
    ):
        num_freqs = responses.shape[-1] // 2 + 1
        if asds.shape[-1] != num_freqs:
            asds = torch.nn.functional.interpolate(asds, size=(num_freqs,))
        snrs = gw.compute_network_snr(
            responses, asds**2, self.sample_rate, self.mask
        )
        if target_snrs is None:
            idx = torch.randperm(len(snrs))
            target_snrs = snrs[idx]

        target_snrs.to(snrs.device)
        weights = target_snrs / snrs
        rescaled_responses = responses * weights.view(-1, 1, 1)

        return rescaled_responses, target_snrs


class SnrSampler:
    """
    Randomly sample values from a power law distribution,
    initially defined with a minimum of `max_min_snr`, a
    maximum of `max_snr`, and an exponent of `alpha` (see
    `ml4gw.distributions.PowerLaw` for details). The
    distribution will gradually change to have a minimum
    of `min_min_snr` over the course of `decay_steps` steps.

    The ending distribution was chosen as an approximate
    empirical match to the SNR distribution of signals
    generated by `aframe.priors.end_o3_ratesandpops` and
    injected in O3 noise. This curriculum training of
    SNRs is intended to aid the network in learning
    low SNR events.
    """

    def __init__(
        self,
        max_min_snr: float,
        min_min_snr: float,
        max_snr: float,
        alpha: float,
        decay_steps: int,
    ):
        self.max_min_snr = max_min_snr
        self.min_min_snr = min_min_snr
        self.max_snr = max_snr
        self.alpha = alpha
        self.decay_steps = decay_steps
        self._step = 0

        self.dist = PowerLaw(max_min_snr, max_snr, alpha)

    def __call__(self, N):
        return self.dist(N)

    def step(self):
        self._step += 1
        if self._step > self.decay_steps:
            return

        frac = self._step / self.decay_steps
        diff = self.max_min_snr - self.min_min_snr
        new = self.max_min_snr - frac * diff

        self.dist.x_min = new
        self.dist.normalization = new ** (-self.alpha + 1)
        self.dist.normalization -= self.max_snr ** (-self.alpha + 1)


class MultiResolutionSpectrogram(torch.nn.Module):
    """
    Create a single spectrogram that combines information
    from multiple spectrograms of the same timeseries.
    Input is expected to have the shape `(B, C, T)`,
    where `B` is the number of batches, `C` is the number
    of channels, and `T` is the number of time samples.

    Given a list of `n_fft`s, calculate the spectrogram
    corresponding to each and combine them by taking the
    maximum value from each bin, which has been normalized.

    If the largest number of time bins among the spectrograms
    is `N` and the largest number of frequency bins is `M`,
    the output will have dimensions `(B, C, M, N)`
    """

    def __init__(
        self,
        n_ffts: torch.Tensor,
        sample_rate: float,
        kernel_length: float,
    ):
        super().__init__()
        self.transforms = [
            Spectrogram(n_fft, normalized=True) for n_fft in n_ffts
        ]
        self.num_freqs = int(max(n_ffts // 2 + 1))
        self.num_times = int(
            max(kernel_length * sample_rate // (n_ffts // 2) + 1)
        )

    def forward(self, X):
        spectrograms = [
            (transform.to(X.device))(X) for transform in self.transforms
        ]

        time_idxs = torch.tensor(
            [
                [
                    int(i * spec.shape[-1] / self.num_times)
                    for spec in spectrograms
                ]
                for i in range(self.num_times)
            ]
        )
        time_idxs = time_idxs.repeat(self.num_freqs, 1, 1)
        freq_idxs = torch.tensor(
            [
                [
                    int(i * spec.shape[-2] / self.num_freqs)
                    for spec in spectrograms
                ]
                for i in range(self.num_freqs)
            ]
        )
        freq_idxs = freq_idxs.repeat(self.num_times, 1, 1).transpose(0, 1)

        stacked_specs = torch.stack(
            [
                spec[:, :, freq_idxs[..., i], time_idxs[..., i]]
                for i, spec in enumerate(spectrograms)
            ],
            axis=-1,
        )
        return torch.max(stacked_specs, axis=-1)[0]
